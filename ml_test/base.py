import time
import json
import numpy as np
from typing import Dict, List, Tuple, Callable, Any, Optional, Union
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
import traceback


@dataclass
class TestResult:
    """Store results from a single test run."""
    test_name: str
    converged: bool
    final_loss: float
    iterations: int
    runtime: float
    performance_metrics: Dict[str, float]
    passed: bool
    error_message: Optional[str] = None


@dataclass
class TestCase:
    """Define a test case with expected behavior."""
    name: str
    data_generator: Callable
    expected_convergence: bool
    max_iterations: int
    convergence_threshold: float
    performance_thresholds: Dict[str, float]  # e.g., {'accuracy': 0.95, 'f1_score': 0.9}
    timeout_seconds: float = 300.0


class Algorithm(ABC):
    """Abstract base class for algorithms to be tested."""

    @abstractmethod
    def fit(self, X, y=None, **kwargs):
        """Train the algorithm."""
        pass

    @abstractmethod
    def predict(self, X):
        """Make predictions."""
        pass

    @abstractmethod
    def get_loss_history(self) -> List[float]:
        """Return the loss history for convergence analysis."""
        pass

    @abstractmethod
    def get_performance_metrics(self, X_test, y_test) -> Dict[str, float]:
        """Return performance metrics on test data."""
        pass


class ConvergenceChecker:
    """Check if an algorithm has converged based on loss history."""

    @staticmethod
    def check_convergence(loss_history: List[float],
                          threshold: float,
                          patience: int = 10) -> Tuple[bool, int]:
        """
        Check if algorithm converged based on loss stabilization.

        Args:
            loss_history: List of loss values over iterations
            threshold: Minimum improvement threshold
            patience: Number of iterations to wait for improvement

        Returns:
            (converged, convergence_iteration)
        """
        if len(loss_history) < patience + 1:
            return False, -1

        for i in range(patience, len(loss_history)):
            recent_losses = loss_history[i - patience:i]
            current_loss = loss_history[i]

            # Check if loss has stabilized (no significant improvement)
            min_recent = min(recent_losses)
            if abs(current_loss - min_recent) < threshold:
                return True, i

        return False, -1

    @staticmethod
    def plot_convergence(loss_history: List[float], test_name: str, save_path: str = None):
        """Plot convergence curve."""
        plt.figure(figsize=(10, 6))
        plt.plot(loss_history)
        plt.title(f'Convergence Plot - {test_name}')
        plt.xlabel('Iteration')
        plt.ylabel('Loss')
        plt.grid(True)

        if save_path:
            plt.savefig(save_path)
        plt.show()

    @staticmethod
    def plot_predicts(prediction_future: List[float], target_future: List[float], prediction_history: List[float] = None, target_history: List[float] = None, test_name: str = None, save_path: str = None):
        """Plot predictions over time."""
        if test_name is None:
            raise ValueError('Variable `test_name` must be given.')
        plt.figure(figsize=(10, 6))
        N = len(prediction_history) if prediction_history is not None else 0
        plt.plot(N + np.arange(len(prediction_future)), prediction_future, '.', label='Predictions (future)', color='dodgerblue')
        plt.plot(N + np.arange(len(target_future)), target_future, '.', label='Target (future)', color='gold')
        if prediction_history is not None:
            plt.plot(prediction_history, '.', label='Predictions (history)', color='blue')
        if target_history is not None:
            plt.plot(target_history, '.', label='Target (history)', color='orange')
        if N != 0:
            plt.axvline(N, color='black', label='future')
        plt.title(f'Predictions - {test_name}')
        plt.xlabel('Iteration')
        plt.ylabel('Prediction')
        plt.legend()
        plt.grid(True)

        if save_path:
            plt.savefig(save_path)
        plt.show()


class AlgorithmTester:
    """Main testing framework for AI algorithms."""

    def __init__(self, results_dir: str = "test_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        self.test_history: List[Dict] = []

    def run_single_test(self, algorithm: Algorithm, test_case: TestCase) -> TestResult:
        """Run a single test case on an algorithm."""
        print(f"Running test: {test_case.name}")

        try:
            # Generate test data
            X_train, y_train, X_test, y_test = test_case.data_generator()

            # Time the training
            start_time = time.time()

            # Train with timeout protection
            algorithm.fit(X_train, y_train, max_iterations=test_case.max_iterations)

            runtime = time.time() - start_time

            # Check for timeout
            if runtime > test_case.timeout_seconds:
                return TestResult(
                    test_name=test_case.name,
                    converged=False,
                    final_loss=float('inf'),
                    iterations=test_case.max_iterations,
                    runtime=runtime,
                    performance_metrics={},
                    passed=False,
                    error_message=f"Timeout: {runtime:.2f}s > {test_case.timeout_seconds}s"
                )

            # Get loss history and check convergence
            loss_history = algorithm.get_loss_history()
            converged, conv_iter = ConvergenceChecker.check_convergence(
                loss_history, test_case.convergence_threshold
            )

            # Get predictions and target history
            if hasattr(algorithm, 'predict_history'):
                # If the algorithm has a predict_history method, use it
                print("Using predict_history for predictions.")
                prediction_history = algorithm.predict_history
                target_history = y_train.tolist()  # Assuming y_train is the target
            else:
                # Otherwise, just predict on the test set
                print("Using predict method for predictions.")
                prediction_history = algorithm.predict(X_train).tolist()
                target_history = y_train.tolist()  # Assuming y_train is the target

            prediction_future = algorithm.predict(X_test)
            target_future = y_test.tolist()

            # Calculate performance metrics
            performance_metrics = algorithm.get_performance_metrics(X_test, y_test)

            # Check if convergence matches expectation
            convergence_ok = converged == test_case.expected_convergence

            # Check if performance meets thresholds
            performance_ok = all(
                performance_metrics.get(metric, 0) >= threshold
                for metric, threshold in test_case.performance_thresholds.items()
            )

            # Overall pass/fail
            passed = convergence_ok and performance_ok

            # Save convergence plot
            plot_path = self.results_dir / f"{test_case.name}_convergence.png"
            ConvergenceChecker.plot_convergence(loss_history, test_case.name, str(plot_path))

            # Save prediction plot if applicable
            plot_path = self.results_dir / f"{test_case.name}_predictions.png"
            ConvergenceChecker.plot_predicts(prediction_future, target_future, prediction_history, target_history, test_case.name, str(plot_path))

            return TestResult(
                test_name=test_case.name,
                converged=converged,
                final_loss=loss_history[-1] if loss_history else float('inf'),
                iterations=len(loss_history),
                runtime=runtime,
                performance_metrics=performance_metrics,
                passed=passed
            )

        except Exception as e:
            return TestResult(
                test_name=test_case.name,
                converged=False,
                final_loss=float('inf'),
                iterations=0,
                runtime=0.0,
                performance_metrics={},
                passed=False,
                error_message=''.join(traceback.format_exception(type(e), e, e.__traceback__)),
            )

    def run_test_suite(self, algorithm: Union[Algorithm, Callable], test_cases: List[TestCase]) -> List[TestResult]:
        """Run a complete test suite."""
        results = []

        print(f"Running {len(test_cases)} test cases...")
        print("=" * 50)

        for test_case in test_cases:
            if isinstance(algorithm, Callable):
                # If algorithm is a callable (e.g., function), instantiate it
                algo = algorithm()
            else:
                algo = algorithm  # Already an instance
            result = self.run_single_test(algo, test_case)
            results.append(result)

            # Print immediate feedback
            status = "âœ“ PASS" if result.passed else "âœ— FAIL"
            print(f"{status} {test_case.name} - {result.runtime:.2f}s")
            if not result.passed and result.error_message:
                # Print traceback
                print(f"   Error: {result.error_message}")

        # Save results
        self._save_results(results)
        self._print_summary(results)

        return results

    def _save_results(self, results: List[TestResult]):
        """Save test results to JSON file."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = self.results_dir / f"test_results_{timestamp}.json"

        results_data = {
            'timestamp': timestamp,
            'results': [asdict(result) for result in results]
        }

        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2)

        print(f"\nResults saved to: {results_file}")

    def _print_summary(self, results: List[TestResult]):
        """Print test summary."""
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.passed)

        print("\n" + "=" * 50)
        print("TEST SUMMARY")
        print("=" * 50)
        print(f"Total tests: {total_tests}")
        print(f"Passed: {passed_tests}")
        print(f"Failed: {total_tests - passed_tests}")
        print(f"Success rate: {passed_tests / total_tests * 100:.1f}%")

        # Show failed tests
        failed_tests = [r for r in results if not r.passed]
        if failed_tests:
            print("\nFailed tests:")
            for result in failed_tests:
                print(f"  - {result.test_name}: {result.error_message or 'Performance/convergence issue'}")


# Example data generators for common test cases
class DataGenerators:
    """Common data generators for testing."""

    @staticmethod
    def switching(n_samples=1000, noise=0.1, n_switches=3):
        """Generate switching cases data."""
        np.random.seed(42)
        X = np.random.randn(n_samples, 2)
        y = np.arange(n_samples) // (n_samples//n_switches)  # Alternating labels

        # Add some noise
        noise_mask = np.random.random(n_samples) < noise
        y[noise_mask] = np.random.choice(n_switches, size=noise_mask.sum())

        # Split train/test
        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def constant0(n_samples=1000, noise=0.1):
        """Generate linearly separable 2D data."""
        np.random.seed(42)
        X = np.random.randn(n_samples, 2)
        y = (X[:, 0] *0. ).astype(int)

        # Add some noise
        noise_mask = np.random.random(n_samples) < noise
        y[noise_mask] = 1 + y[noise_mask]

        # Split train/test
        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def constant1(n_samples=1000, noise=0.1):
        """Generate linearly separable 2D data."""
        np.random.seed(42)
        X = np.random.randn(n_samples, 2)
        y = (X[:, 0] *0. + 1.).astype(int)

        # Add some noise
        noise_mask = np.random.random(n_samples) < noise
        y[noise_mask] = 1 - y[noise_mask]

        # Split train/test
        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def linear_2d(n_samples=1000, noise=0.1):
        """Generate linearly separable 2D data."""
        np.random.seed(42)
        X = np.random.randn(n_samples, 2)
        y = X[:, 0] + X[:, 1]

        # Add some noise
        y = y + noise * np.random.randn(n_samples)

        # Split train/test
        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def linear_separable_2d(n_samples=1000, noise=0.1):
        """Generate linearly separable 2D data."""
        np.random.seed(42)
        X = np.random.randn(n_samples, 2)
        y = (X[:, 0] + X[:, 1] > 0).astype(int)

        # Add some noise
        noise_mask = np.random.random(n_samples) < noise
        y[noise_mask] = 1 - y[noise_mask]

        # Split train/test
        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def xor_problem(n_samples=1000):
        """Generate XOR problem data (not linearly separable)."""
        np.random.seed(42)
        X = np.random.uniform(-1, 1, (n_samples, 2))
        y = ((X[:, 0] > 0) != (X[:, 1] > 0)).astype(int)

        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def regression_polynomial(n_samples=1000, degree=2, noise=0.1):
        """Generate polynomial regression data."""
        np.random.seed(42)
        X = np.random.uniform(-2, 2, (n_samples, 1))
        y = np.sum([X ** i for i in range(1, degree + 1)], axis=0).flatten()
        y += noise * np.random.randn(n_samples)

        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def timeseries_sine(n_samples=1000, noise=0.1, cycles=5):
        """Generate sine wave time series data."""
        np.random.seed(42)
        X = np.linspace(0, 2 * np.pi * cycles, n_samples).reshape(-1, 1)
        y = np.sin(X).flatten() + noise * np.random.randn(n_samples) + 1

        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]

    @staticmethod
    def timeseries_linear(n_samples=1000, noise=0.1, start=0, end=10):
        """Generate linear time series data."""
        np.random.seed(42)
        X = np.arange(n_samples).reshape(-1, 1)
        y = np.linspace(start, end, n_samples) + noise * np.random.randn(n_samples)

        split_idx = int(0.8 * n_samples)
        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]


# Example usage and test cases
def create_example_test_cases():
    """Create example test cases."""
    return [
        TestCase(
            name="linear_separable_convergence",
            data_generator=DataGenerators.linear_separable_2d,
            expected_convergence=True,
            max_iterations=1000,
            convergence_threshold=1e-6,
            performance_thresholds={'accuracy': 0.85},
            timeout_seconds=30.0
        ),
        TestCase(
            name="xor_problem_should_not_converge_linear",
            data_generator=DataGenerators.xor_problem,
            expected_convergence=False,  # Linear algorithm shouldn't solve XOR
            max_iterations=1000,
            convergence_threshold=1e-6,
            performance_thresholds={'accuracy': 0.45},  # Random performance expected
            timeout_seconds=30.0
        ),
        TestCase(
            name="polynomial_regression",
            data_generator=DataGenerators.regression_polynomial,
            expected_convergence=True,
            max_iterations=1000,
            convergence_threshold=1e-4,
            performance_thresholds={'r2_score': 0.8},
            timeout_seconds=60.0
        )
    ]


# Sklearn implementations for testing
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error


class SklearnDecisionTreeClassifier(Algorithm):
    """Sklearn DecisionTree wrapper for classification."""

    def __init__(self, **kwargs):
        self.model = DecisionTreeClassifier(random_state=42, **kwargs)
        self.loss_history = []
        self.is_fitted = False

    def fit(self, X, y=None, **kwargs):
        """Train the decision tree."""
        max_iterations = kwargs.get('max_iterations', 1000)  # Not used for trees, but kept for interface

        # Decision trees don't have iterative training, so we simulate loss history
        # by tracking training accuracy improvement during tree construction
        self.model.fit(X, y)
        self.is_fitted = True

        # Simulate convergence behavior for decision trees
        # Trees typically "converge" in one step, but we'll create a realistic loss curve
        train_pred = self.model.predict(X)
        train_accuracy = accuracy_score(y, train_pred)
        train_loss = 1 - train_accuracy

        # Create a simulated loss history showing rapid convergence
        self.loss_history = [
            0.5,  # Random starting point
            0.3,  # Rapid improvement
            0.15,  # More improvement
            train_loss,  # Final training loss
            train_loss,  # Converged
            train_loss  # Stays converged
        ]

    def predict(self, X):
        """Make predictions."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")
        return self.model.predict(X)

    def get_loss_history(self) -> List[float]:
        """Return the loss history."""
        return self.loss_history

    def get_performance_metrics(self, X_test, y_test) -> Dict[str, float]:
        """Return performance metrics on test data."""
        if not self.is_fitted:
            return {}

        y_pred = self.predict(X_test)

        # For classification
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')

        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'error_rate': 1 - accuracy
        }


class SklearnDecisionTreeRegressor(Algorithm):
    """Sklearn DecisionTree wrapper for regression."""

    def __init__(self, **kwargs):
        self.model = DecisionTreeRegressor(random_state=42, **kwargs)
        self.loss_history = []
        self.is_fitted = False

    def fit(self, X, y=None, **kwargs):
        """Train the decision tree regressor."""
        self.model.fit(X, y)
        self.is_fitted = True

        # Simulate loss history for regression
        train_pred = self.model.predict(X)
        train_mse = mean_squared_error(y, train_pred)

        # Create a simulated loss history
        initial_mse = np.var(y)  # Worst case: predicting mean
        self.loss_history = [
            initial_mse,
            initial_mse * 0.7,
            initial_mse * 0.4,
            initial_mse * 0.2,
            train_mse,
            train_mse
        ]

    def predict(self, X):
        """Make predictions."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")
        return self.model.predict(X)

    def get_loss_history(self) -> List[float]:
        """Return the loss history."""
        return self.loss_history

    def get_performance_metrics(self, X_test, y_test) -> Dict[str, float]:
        """Return performance metrics on test data."""
        if not self.is_fitted:
            return {}

        y_pred = self.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        return {
            'mse': mse,
            'rmse': np.sqrt(mse),
            'r2_score': r2
        }


def create_decision_tree_test_cases():
    """Create test cases specifically for decision trees."""
    return [
        TestCase(
            name="dt_linear_separable",
            data_generator=DataGenerators.linear_separable_2d,
            expected_convergence=True,
            max_iterations=100,  # Not really used for trees
            convergence_threshold=1e-3,
            performance_thresholds={'accuracy': 0.90, 'f1_score': 0.85},
            timeout_seconds=10.0
        ),
        TestCase(
            name="dt_xor_problem",
            data_generator=DataGenerators.xor_problem,
            expected_convergence=True,  # Trees should handle XOR well
            max_iterations=100,
            convergence_threshold=1e-3,
            performance_thresholds={'accuracy': 0.85, 'f1_score': 0.80},
            timeout_seconds=10.0
        ),
        TestCase(
            name="dt_polynomial_regression",
            data_generator=DataGenerators.regression_polynomial,
            expected_convergence=True,
            max_iterations=100,
            convergence_threshold=1e-3,
            performance_thresholds={'r2_score': 0.75},
            timeout_seconds=15.0
        )
    ]


if __name__ == "__main__":
    print("Testing DecisionTree with AI Algorithm Testing Framework")
    print("=" * 60)

    # Test Classification
    print("\nðŸŒ³ Testing DecisionTree Classifier...")
    classifier = SklearnDecisionTreeClassifier(max_depth=10)
    tester = AlgorithmTester(results_dir="dt_test_results")

    # Create classification test cases (exclude regression)
    classification_tests = [tc for tc in create_decision_tree_test_cases()
                            if 'regression' not in tc.name]

    results_classification = tester.run_test_suite(classifier, classification_tests)

    # Test Regression
    print("\nðŸŒ³ Testing DecisionTree Regressor...")
    regressor = SklearnDecisionTreeRegressor(max_depth=10)

    # Create regression test cases
    regression_tests = [tc for tc in create_decision_tree_test_cases()
                        if 'regression' in tc.name]

    results_regression = tester.run_test_suite(regressor, regression_tests)

    print("\nâœ… All tests completed! Check the 'dt_test_results' directory for detailed results and plots.")