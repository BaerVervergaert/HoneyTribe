{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Algorithms Guide\n",
    "This notebook provides a comprehensive guide to using the k-means clustering algorithms implemented in HoneyTribe.\n",
    "## Overview\n",
    "The following algorithms are covered:\n",
    "1. **WeightedKMeans** - Standard weighted K-means clustering\n",
    "2. **InverseWeightedKMeans** - Inverse weighted variant with configurable parameters\n",
    "3. **OnlineKMeans** - Online learning variant with incremental updates\n",
    "4. **InverseWeightedOnlineKMeansV1** - Online variant with inverse weighting (v1)\n",
    "5. **InverseWeightedOnlineKMeansV2** - Online variant with inverse weighting (v2)\n",
    "6. **KHarmonicMeans** - Harmonic means variant\n",
    "\n",
    "## Contents\n",
    "1. Installation and imports\n",
    "2. Creating sample data\n",
    "3. Using each algorithm\n",
    "4. Comparing algorithm performance\n",
    "5. Visualization and interpretation\n",
    "6. Advanced usage patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages (if needed)\n",
    "!pip install numpy scikit-learn matplotlib pandas\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import k-means algorithms\n",
    "from honeytribe.taxonomy.k_means import (\n",
    "    WeightedKMeans,\n",
    "    InverseWeightedKMeans,\n",
    "    OnlineKMeans,\n",
    "    InverseWeightedOnlineKMeansV1,\n",
    "    InverseWeightedOnlineKMeansV2,\n",
    "    KHarmonicMeans,\n",
    ")\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Sample DataLet's create synthetic data for demonstrating the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Create synthetic data with 3 clusters\n",
    "n_samples_per_cluster = 50\n",
    "n_clusters = 3\n",
    "# Cluster 1: centered at (0, 0)\n",
    "cluster1 = np.random.randn(n_samples_per_cluster, 2) * 0.5 + np.array([0, 0])\n",
    "# Cluster 2: centered at (5, 5)\n",
    "cluster2 = np.random.randn(n_samples_per_cluster, 2) * 0.5 + np.array([5, 5])\n",
    "# Cluster 3: centered at (2, 7)\n",
    "cluster3 = np.random.randn(n_samples_per_cluster, 2) * 0.5 + np.array([2, 7])\n",
    "# Combine all clusters\n",
    "X = np.vstack([cluster1, cluster2, cluster3])\n",
    "# Create true labels for comparison\n",
    "y_true = np.array([0] * n_samples_per_cluster +\n",
    "                 [1] * n_samples_per_cluster +\n",
    "                 [2] * n_samples_per_cluster)\n",
    "\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y_true = y_true[permutation]\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Samples per cluster: {n_samples_per_cluster}\")\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "plt.colorbar(scatter, label='True Cluster')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Data with 3 Clusters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"âœ“ Data created successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Individual Algorithms\n",
    "### 3.1 WeightedKMeans"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize and fit WeightedKMeans\n",
    "weighted_kmeans = WeightedKMeans(n_clusters=3, max_iter=100, tol=1e-4)\n",
    "weighted_kmeans.fit(X)\n",
    "# Make predictions\n",
    "labels_weighted = weighted_kmeans.predict(X)\n",
    "# Get cluster centers\n",
    "centers_weighted = weighted_kmeans.cluster_centers_\n",
    "# Compute score\n",
    "score_weighted = weighted_kmeans.score(X)\n",
    "print(\"WeightedKMeans Results:\")\n",
    "print(f\"  Score: {score_weighted:.4f}\")\n",
    "print(f\"  Cluster centers shape: {centers_weighted.shape}\")\n",
    "print(f\"  Unique labels: {np.unique(labels_weighted)}\")\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# Plot 1: True labels\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('True Labels')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Plot 2: Predicted labels\n",
    "scatter = axes[1].scatter(X[:, 0], X[:, 1], c=labels_weighted, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "axes[1].scatter(centers_weighted[:, 0], centers_weighted[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centers')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('WeightedKMeans Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 InverseWeightedKMeans"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize and fit InverseWeightedKMeans\n",
    "# The n and p parameters control the algorithm behavior\n",
    "# Constraint: n >= p and n <= p + 2\n",
    "iwk = InverseWeightedKMeans(n_clusters=3, max_iter=100, tol=1e-4, n=3, p=1)\n",
    "iwk.fit(X)\n",
    "labels_iwk = iwk.predict(X)\n",
    "centers_iwk = iwk.cluster_centers_\n",
    "score_iwk = iwk.score(X)\n",
    "print(\"InverseWeightedKMeans Results:\")\n",
    "print(f\"  Score: {score_iwk:.4f}\")\n",
    "print(f\"  Parameters: n={iwk.n}, p={iwk.p}\")\n",
    "print(f\"  Cluster centers shape: {centers_iwk.shape}\")\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels_iwk, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "plt.scatter(centers_iwk[:, 0], centers_iwk[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centers')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('InverseWeightedKMeans Clustering')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 OnlineKMeans (Incremental Learning)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OnlineKMeans supports incremental learning\n",
    "online_kmeans = OnlineKMeans(n_clusters=3, learning_rate=0.1, n_iter=5)\n",
    "# Method 1: Fit with entire dataset at once\n",
    "online_kmeans.fit(X)\n",
    "labels_online = online_kmeans.predict(X)\n",
    "centers_online = online_kmeans.cluster_centers_\n",
    "score_online = online_kmeans.score(X)\n",
    "print(\"OnlineKMeans Results:\")\n",
    "print(f\"  Score: {score_online:.4f}\")\n",
    "print(f\"  Learning rate: {online_kmeans.learning_rate}\")\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels_online, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "plt.scatter(centers_online[:, 0], centers_online[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centers')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('OnlineKMeans Clustering')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 OnlineKMeans with Incremental Updates (Streaming Data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OnlineKMeans also supports incremental learning via partial_fit\n",
    "# This is useful for streaming data or mini-batch learning\n",
    "online_kmeans_incremental = OnlineKMeans(n_clusters=3, learning_rate=0.05, n_iter=1)\n",
    "# Simulate streaming data: process in batches\n",
    "batch_size = 10\n",
    "n_batches = len(X) // batch_size\n",
    "scores_history = []\n",
    "for batch_idx in range(n_batches):\n",
    "    # Get batch\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch = X[start_idx:end_idx]\n",
    "    # Update model\n",
    "    online_kmeans_incremental.partial_fit(batch)\n",
    "    # Compute score on entire dataset\n",
    "    score = online_kmeans_incremental.score(X)\n",
    "    scores_history.append(score)\n",
    "    labels_incremental = online_kmeans_incremental.predict(X)\n",
    "    print(\"OnlineKMeans Incremental Learning Results:\")\n",
    "    print(f\"  Final score: {scores_history[-1]:.4f}\")\n",
    "    print(f\"  Number of batches processed: {n_batches}\")\n",
    "    print(f\"  Score improvement: {abs(scores_history[-1] - scores_history[0]):.4f}\")\n",
    "    # Plot convergence\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    # Plot 1: Score over batches\n",
    "    axes[0].plot(scores_history, 'b-o', linewidth=2, markersize=6)\n",
    "    axes[0].set_xlabel('Batch Number')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('OnlineKMeans: Score Convergence Over Batches')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    # Plot 2: Final clustering\n",
    "    scatter = axes[1].scatter(X[:, 0], X[:, 1], c=labels_incremental, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "    axes[1].scatter(online_kmeans_incremental.cluster_centers_[:, 0],\n",
    "                    online_kmeans_incremental.cluster_centers_[:, 1],\n",
    "                    c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centers')\n",
    "    axes[1].set_xlabel('Feature 1')\n",
    "    axes[1].set_ylabel('Feature 2')\n",
    "    axes[1].set_title('Final OnlineKMeans Clustering')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 InverseWeightedOnlineKMeansV2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# InverseWeightedOnlineKMeansV2 - improved online variant\n",
    "iw_online_v2 = InverseWeightedOnlineKMeansV2(n_clusters=3, learning_rate=0.01, n_iter=5, n=2, p=2)\n",
    "iw_online_v2.fit(X)\n",
    "labels_v2 = iw_online_v2.predict(X)\n",
    "centers_v2 = iw_online_v2.cluster_centers_\n",
    "score_v2 = iw_online_v2.score(X)\n",
    "print(\"InverseWeightedOnlineKMeansV2 Results:\")\n",
    "print(f\"  Score: {score_v2:.4f}\")\n",
    "print(f\"  Parameters: n={iw_online_v2.n}, p={iw_online_v2.p}\")\n",
    "print(f\"  Learning rate: {iw_online_v2.learning_rate}\")\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels_v2, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "plt.scatter(centers_v2[:, 0], centers_v2[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centers')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('InverseWeightedOnlineKMeansV2 Clustering')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 KHarmonicMeans"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# K-Harmonic Means - robust clustering algorithm\n",
    "khm = KHarmonicMeans(n_clusters=3, learning_rate=0.0001, n_iter=5)\n",
    "khm.fit(X)\n",
    "labels_khm = khm.predict(X)\n",
    "centers_khm = khm.cluster_centers_\n",
    "score_khm = khm.score(X)\n",
    "print(\"KHarmonicMeans Results:\")\n",
    "print(f\"  Score: {score_khm:.4f}\")\n",
    "print(f\"  Learning rate: {khm.learning_rate}\")\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels_khm, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
    "plt.scatter(centers_khm[:, 0], centers_khm[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centers')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('KHarmonicMeans Clustering')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Algorithm PerformanceLet's compare all algorithms on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dictionary to store resultsresults = {}# 1. WeightedKMeanswk = WeightedKMeans(n_clusters=3, max_iter=100)wk.fit(X)results['WeightedKMeans'] = {    'score': wk.score(X),    'labels': wk.predict(X),    'centers': wk.cluster_centers_},# 2. InverseWeightedKMeansiwk = InverseWeightedKMeans(n_clusters=3, max_iter=100)iwk.fit(X)results['InverseWeightedKMeans'] = {    'score': iwk.score(X),    'labels': iwk.predict(X),    'centers': iwk.cluster_centers_},# 3. OnlineKMeansokm = OnlineKMeans(n_clusters=3, learning_rate=0.1)okm.fit(X)results['OnlineKMeans'] = {    'score': okm.score(X),    'labels': okm.predict(X),    'centers': okm.cluster_centers_},# 4. InverseWeightedOnlineKMeansV2iw_v2 = InverseWeightedOnlineKMeansV2(n_clusters=3, learning_rate=0.05)iw_v2.fit(X)results['IWOnlineKMeansV2'] = {    'score': iw_v2.score(X),    'labels': iw_v2.predict(X),    'centers': iw_v2.cluster_centers_},# 5. KHarmonicMeanskhm = KHarmonicMeans(n_clusters=3, learning_rate=0.05)khm.fit(X)results['KHarmonicMeans'] = {    'score': khm.score(X),    'labels': khm.predict(X),    'centers': khm.cluster_centers_},# Create comparison tablecomparison_df = pd.DataFrame({    'Algorithm': list(results.keys()),    'Score': [results[alg]['score'] for alg in results.keys()]})comparison_df = comparison_df.sort_values('Score', ascending=False)print(\"Algorithm Performance Comparison:\")print(comparison_df.to_string(index=False))print(f\"Best algorithm: {comparison_df.iloc[0]['Algorithm']} (Score: {comparison_df.iloc[0]['Score']:.4f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Compare All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a grid of subplots for all algorithmsfig, axes = plt.subplots(2, 3, figsize=(16, 10))axes = axes.flatten()algorithm_names = list(results.keys())for idx, (ax, alg_name) in enumerate(zip(axes, algorithm_names)):    labels = results[alg_name]['labels']    centers = results[alg_name]['centers']    score = results[alg_name]['score']        # Scatter plot    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=80, alpha=0.6, edgecolors='k')        # Plot centers    ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=250, edgecolors='black', linewidth=1.5)        ax.set_xlabel('Feature 1', fontsize=10)    ax.set_ylabel('Feature 2', fontsize=10)    ax.set_title(f'{alg_name}Score: {score:.2f}', fontsize=11, fontweight='bold')    ax.grid(True, alpha=0.3)# Hide the last subplotaxes[-1].axis('off')plt.suptitle('K-Means Algorithm Comparison', fontsize=14, fontweight='bold', y=0.995)plt.tight_layout()plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bar chart comparing scoresfig, ax = plt.subplots(figsize=(12, 6))algorithms = list(results.keys())scores = [results[alg]['score'] for alg in algorithms]colors = plt.cm.viridis(np.linspace(0, 1, len(algorithms)))bars = ax.bar(algorithms, scores, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)# Add value labels on barsfor bar, score in zip(bars, scores):    height = bar.get_height()    ax.text(bar.get_x() + bar.get_width()/2., height,            f'{score:.2f}',            ha='center', va='bottom', fontsize=10, fontweight='bold')ax.set_ylabel('Score (Negative Sum of Squared Distances)', fontsize=11)ax.set_xlabel('Algorithm', fontsize=11)ax.set_title('Algorithm Performance Comparison', fontsize=12, fontweight='bold')ax.grid(True, alpha=0.3, axis='y')plt.xticks(rotation=45, ha='right')plt.tight_layout()plt.show()print(\"Score Interpretation:\")print(\"- Higher scores (less negative) indicate better clustering\")print(\"- Score = -sum of squared distances to nearest cluster center\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Usage Patterns\n",
    "### 5.1 Trying Different Numbers of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Elbow method: find optimal number of clusters\n",
    "cluster_range = range(1, 8)\n",
    "scores_by_clusters = {}\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = WeightedKMeans(n_clusters=n_clusters, max_iter=100)\n",
    "    kmeans.fit(X)\n",
    "    score = kmeans.score(X)\n",
    "    scores_by_clusters[n_clusters] = score\n",
    "    print(f\"n_clusters={n_clusters}: score={score:.4f}\")\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "clusters = list(scores_by_clusters.keys())\n",
    "scores = list(scores_by_clusters.values())\n",
    "plt.plot(clusters, scores, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=3, color='r', linestyle='--', linewidth=2, label='True number of clusters')\n",
    "plt.xlabel('Number of Clusters', fontsize=11)\n",
    "plt.ylabel('Score', fontsize=11)\n",
    "plt.title('Elbow Method: Finding Optimal Number of Clusters', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Optimal number of clusters (by elbow method): ~3\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Effect of Learning Rate (Online Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "scores_by_lr = {}\n",
    "for lr in learning_rates:\n",
    "    kmeans = OnlineKMeans(n_clusters=3, learning_rate=lr, n_iter=10)\n",
    "    kmeans.fit(X)\n",
    "    score = kmeans.score(X)\n",
    "    scores_by_lr[lr] = score\n",
    "    print(f\"learning_rate={lr}: score={score:.4f}\")\n",
    "# Plot learning rate effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "lrs = list(scores_by_lr.keys())\n",
    "scores = list(scores_by_lr.values())\n",
    "plt.plot(lrs, scores, 'go-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Learning Rate', fontsize=11)\n",
    "plt.ylabel('Score', fontsize=11)\n",
    "plt.title('Effect of Learning Rate on OnlineKMeans Performance', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Effect of Parameters (InverseWeightedKMeans)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare different n, p parameter combinations# Constraint: n >= p and n <= p + 2param_combinations = [(1, 1), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3)]scores_by_params = {}for n, p in param_combinations:    if n >= p and n <= p + 2:  # Check constraints        kmeans = InverseWeightedKMeans(n_clusters=3, max_iter=100, n=n, p=p)        kmeans.fit(X)        score = kmeans.score(X)        scores_by_params[(n, p)] = score        print(f\"n={n}, p={p}: score={score:.4f}\")# Visualize parameter effectsfig, ax = plt.subplots(figsize=(12, 6))param_labels = [f'n={n}, p={p}' for n, p in scores_by_params.keys()]param_scores = list(scores_by_params.values())colors = plt.cm.viridis(np.linspace(0, 1, len(param_labels)))bars = ax.bar(param_labels, param_scores, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)# Add value labelsfor bar, score in zip(bars, param_scores):    height = bar.get_height()    ax.text(bar.get_x() + bar.get_width()/2., height,            f'{score:.2f}',            ha='center', va='bottom', fontsize=9, fontweight='bold')ax.set_ylabel('Score', fontsize=11)ax.set_xlabel('Parameters (n, p)', fontsize=11)ax.set_title('InverseWeightedKMeans: Effect of Parameters n and p', fontsize=12, fontweight='bold')ax.grid(True, alpha=0.3, axis='y')plt.xticks(rotation=45, ha='right')plt.tight_layout()plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Handling Higher-Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create higher-dimensional data (10D)\n",
    "np.random.seed(42)\n",
    "n_features = 1000\n",
    "n_samples_per_cluster = 50\n",
    "cluster1_hd = np.random.randn(n_samples_per_cluster, n_features) * 0.5 + 0\n",
    "cluster2_hd = np.random.randn(n_samples_per_cluster, n_features) * 0.5 + 5\n",
    "cluster3_hd = np.random.randn(n_samples_per_cluster, n_features) * 0.5 + 3\n",
    "X_hd = np.vstack([cluster1_hd, cluster2_hd, cluster3_hd])\n",
    "y_true_hd = np.array([0] * n_samples_per_cluster + [1] * n_samples_per_cluster + [2] * n_samples_per_cluster)\n",
    "\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X_hd = X_hd[permutation]\n",
    "y_true_hd = y_true_hd[permutation]\n",
    "\n",
    "print(f\"High-dimensional data shape: {X_hd.shape}\")\n",
    "# Test all algorithms on high-dimensional data\n",
    "hd_results = {}\n",
    "algorithms_hd = [\n",
    "    ('WeightedKMeans', WeightedKMeans(n_clusters=3, max_iter=1000)),\n",
    "    ('InverseWeightedKMeans', InverseWeightedKMeans(n_clusters=3, max_iter=1000)),\n",
    "    ('OnlineKMeans', OnlineKMeans(n_clusters=3, learning_rate=0.1, n_iter=10)),\n",
    "    ('InverseWeightedOnlineKMeansV2', InverseWeightedOnlineKMeansV2(n_clusters=3, learning_rate=0.05)),\n",
    "    ('KHarmonicMeans', KHarmonicMeans(n_clusters=3, learning_rate=0.05)),\n",
    "]\n",
    "\n",
    "for name, model in algorithms_hd:\n",
    "    model.fit(X_hd)\n",
    "    score = model.score(X_hd)\n",
    "    hd_results[name] = score\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "alg_names = list(hd_results.keys())\n",
    "alg_scores = list(hd_results.values())\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(alg_names)))\n",
    "bars = ax.bar(alg_names, alg_scores, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "for bar, score in zip(bars, alg_scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.2f}',\n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=11)\n",
    "ax.set_title(f'Algorithm Performance on High-Dimensional Data ({n_features}D)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways and Best Practices\n",
    "### Algorithm Selection Guide\n",
    "| Algorithm | Use Case | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **WeightedKMeans** | General clustering | Balances local and global interactions | May not find global optimum |\n",
    "| **InverseWeightedKMeans** | Inverse distance weighting | Configurable parameters (n, p) | More complex to tune |\n",
    "| **OnlineKMeans** | Streaming/incremental data | Efficient for large datasets | Requires learning rate tuning |\n",
    "| **InverseWeightedOnlineKMeansV2** | Online with inverse weighting | Combines online and weighting | More computation per step |\n",
    "| **KHarmonicMeans** | Robust clustering | Less sensitive to initialization | More complex algorithm |\n",
    "### Best Practices\n",
    "1. **Data Preprocessing**\n",
    "    - Normalize/standardize features to have similar scales\n",
    "    - Remove outliers that might skew cluster centers\n",
    "    - Handle missing values appropriately\n",
    "2. **Choosing Number of Clusters**\n",
    "    - Use the elbow method\n",
    "    - Try multiple values and compare results\n",
    "    - Use domain knowledge if available\n",
    "3. **Algorithm Selection**\n",
    "    - Start with WeightedKMeans for simplicity\n",
    "    - Use OnlineKMeans for large/streaming datasets\n",
    "    - Try multiple algorithms and compare scores\n",
    "4. **Hyperparameter Tuning**\n",
    "    - For online algorithms: adjust learning rate (0.01-0.5)\n",
    "    - For InverseWeightedKMeans: tune n and p (constraint: n >= p, n <= p+2)\n",
    "    - For batch algorithms: increase max_iter if needed\n",
    "5. **Evaluation**\n",
    "    - Compare scores across algorithms\n",
    "    - Visualize results to ensure they make sense\n",
    "    - Use domain expertise to validate clustering\n",
    "### Key Parameters\n",
    "```python\n",
    "# Common parameters across algorithmsn_clusters\n",
    "# Number of clusters (required)\n",
    "max_iter            # Max iterations for batch algorithms\n",
    "learning_rate       # Step size for online algorithms (default: 0.05)\n",
    "# Specific parameters\n",
    "tol                 # Convergence tolerance (default: 1e-4)\n",
    "n, p                # InverseWeighted parameters (constraints: n >= p, n <= p+2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (honeytribe)",
   "language": "python",
   "name": "honeytribe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
